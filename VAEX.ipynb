{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "VAEX.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nverchev/VAEX/blob/main/VAEX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn2BrCp7Uuju"
      },
      "source": [
        "# VAEX\n",
        "This notebook shows how to train VAEX to reproduce the results pubblished in the paper Hierarchical variational autoencoders for visual counterfactuals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83QzymGhUujy",
        "cellView": "form"
      },
      "source": [
        "#@title Libraries\n",
        "#%load_ext autoreload\n",
        "#%autoreload 2\n",
        "import zipfile\n",
        "from glob2 import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import torchvision\n",
        "from torch import optim\n",
        "from torch.distributions.normal import Normal\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from skimage import io\n",
        "from abc import ABCMeta,abstractmethod\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset,DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoK_B6UoUujz"
      },
      "source": [
        "#@title Hyperparameters: { display-mode: \"form\" }\n",
        "batch_size = 32 #@param {type: \"number\"}\n",
        "x_dim= 129 #@param {type: \"number\"}\n",
        "initial_learning_rate = 0.001 #@param {type: \"number\"}\n",
        "weight_decay = 0.00000 #@param {type: \"number\"}\n",
        "n_resolutions = 5 #@param {type: \"number\"}\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSJ6ch854rKK",
        "cellView": "form"
      },
      "source": [
        "#@title Download\n",
        "\n",
        "Download = True #@param {type:\"boolean\"}\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "if Download:\n",
        "  # 1. Authenticate and create the PyDrive client.\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "  ids={\n",
        "  '0B7EVK8r0v71pZjFTYXZWM3FlRnM':'images.zip',\n",
        "  '0B7EVK8r0v71pblRyaVFSWGxPY0U':'attributes.txt',\n",
        "  '0B7EVK8r0v71pY0NSMzRuSXJEVkk':'partition.txt'}\n",
        "\n",
        "  for id, name in ids.items():\n",
        "    downloaded = drive.CreateFile({'id':id})\n",
        "    downloaded.GetContentFile(name)\n",
        "  with zipfile.ZipFile('images.zip', 'r') as zip_ref:\n",
        "      zip_ref.extractall('./')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euBUVn8X6MyO",
        "cellView": "form"
      },
      "source": [
        "#@title Creating the dataset \n",
        "class CelebaDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, transform=None, target_attr = 'Male'):\n",
        "      self.data = data\n",
        "      self.target_attr = target_attr\n",
        "      self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "      id = int(self.data['image_id'][idx])\n",
        "      img_file = 'img_align_celeba/{:06d}.jpg'.format(id)\n",
        "      img=io.imread(img_file)\n",
        "      if self.transform:\n",
        "        img=self.transform(img)\n",
        "      attr = self.data[self.target_attr][idx]\n",
        "      return [img,torch.as_tensor((attr+1)//2,dtype=torch.long)]\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "  \n",
        "\n",
        "class CelebaLoader():\n",
        "\n",
        "    train_val_split=162770\n",
        "    val_test_split=182637\n",
        "\n",
        "    def __init__(self, dataset_class=CelebaDataset, **dataset_args):\n",
        "      self.attributes = self.read_attributes()\n",
        "      self.dataset_class = dataset_class\n",
        "      self.dataset_args = dataset_args\n",
        "\n",
        "    def read_attributes(self):\n",
        "      with open('attributes.txt', 'r') as reader:\n",
        "          attributes = reader.read()\n",
        "      list_attributes = attributes.split('\\n')\n",
        "      attributes_names = ['image_id']+list_attributes[1].split(' ')[:-1]\n",
        "      numpy_attributes = np.empty((202599,41))\n",
        "      for line, row in zip(list_attributes[2:],numpy_attributes):\n",
        "        i = 0\n",
        "        for attr in line.split(' '):\n",
        "          if not attr: \n",
        "            continue #removes empty strings\n",
        "          if i == 0:\n",
        "            attr = attr[:6] # extracting id\n",
        "          row[i] = int(attr)\n",
        "          i += 1\n",
        "      return pd.DataFrame(data=numpy_attributes,columns=attributes_names)\n",
        "\n",
        "  \n",
        "    def get_dataset(self,subset):\n",
        "      if subset == 'train':\n",
        "        subset_data=self.attributes[:self.train_val_split].reset_index()\n",
        "      elif subset == 'val':\n",
        "        subset_data=self.attributes[self.train_val_split:self.val_test_split].reset_index()\n",
        "      elif subset == 'trainval':\n",
        "        subset_data=self.attributes[:self.val_test_split].reset_index()\n",
        "      elif subset == 'test':\n",
        "        subset_data=self.attributes[self.val_test_split:].reset_index()\n",
        "      else:\n",
        "        raise ValueError('Invalid subset name.')\n",
        "      return self.dataset_class(subset_data,**self.dataset_args)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "762-Gcr30h0c",
        "cellView": "form"
      },
      "source": [
        "visualize_sample=162772 #@param {type: \"number\"}\n",
        "img=io.imread('img_align_celeba/{:06d}.jpg'.format(visualize_sample))\n",
        "img=img[35:-35]\n",
        "img=img[:,15:-15]\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis('off') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "v0i5uNbGCCQG"
      },
      "source": [
        "#@title Loading & Splitting\n",
        "transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                transforms.CenterCrop(148),\n",
        "                                transforms.Resize(x_dim, interpolation=3),\n",
        "                                transforms.ToTensor()])\n",
        "\n",
        "\n",
        "loader = CelebaLoader(transform=transform)\n",
        "# Creating datasets:\n",
        "train_dset = loader.get_dataset('train')\n",
        "val_dset = loader.get_dataset('val')\n",
        "#trainval_dset = loader.get_dataset('trainval')\n",
        "test_dset = loader.get_dataset('test')\n",
        "\n",
        "# Creating data loaders:\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dset,batch_size=batch_size,shuffle=True,drop_last=True)            \n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dset,batch_size=batch_size,drop_last=True)            \n",
        "#trainval_loader = torch.utils.data.DataLoader(dataset=trainval_dset,batch_size=batch_size,shuffle=True,drop_last=True)            \n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dset,batch_size=batch_size,drop_last=False)            \n",
        "loader.attributes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "z--BvC0XCCQO"
      },
      "source": [
        "#@title Block Args\n",
        "block_args={\n",
        "    'optim':optim.Adam,\n",
        "    'initial_lr': initial_learning_rate,\n",
        "    'weight_decay':weight_decay,\n",
        "    'train_loader':train_loader,\n",
        "    'device':device,\n",
        "    'val_loader': val_loader,\n",
        "    'test_loader': test_loader,\n",
        "    'batch_size': batch_size\n",
        "    }\n",
        "for k,v in block_args.items():\n",
        "  if not isinstance(v,(type,torch.utils.data.dataloader.DataLoader)):\n",
        "    print(k, ': ', v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "DYXvjevGCCQP"
      },
      "source": [
        "#@title Trainer\n",
        "class Trainer(metaclass=ABCMeta):\n",
        "    losses=['final_loss']\n",
        "    bin='celeba'\n",
        "    data_path = data_path ## where the network is saved\n",
        "    def __init__(self,model,version,device,optim,train_loader,val_loader=None,test_loader=None,**block_args):\n",
        "        self.model = model.to(device)\n",
        "        self.version = version\n",
        "        self.settings = {**model.settings,**block_args,'Optimizer':str(optim)}\n",
        "        self.optimizer = optim(model.parameters(), lr=block_args['initial_lr'], weight_decay=block_args['weight_decay'])\n",
        "        self.device = device\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.train_losses = {loss:[] for loss in self.losses}\n",
        "        self.val_losses = {loss:[] for loss in self.losses}\n",
        "        self.test_losses = {loss:[] for loss in self.losses}\n",
        "        self.test_inputs,self.test_targets,self.test_outputs = [], [], {}\n",
        "        self.epoch = 0\n",
        "        self.converge = 1\n",
        "\n",
        "    def train(self,num_epoch):\n",
        "        print('Version ',self.version)\n",
        "        for _ in range(num_epoch):\n",
        "            self.epoch+=1\n",
        "            print('====> Epoch:{}'.format(self.epoch))        \n",
        "            self._run_session(mode='train')\n",
        "            if self.val_loader:\n",
        "              self._run_session(mode='val')\n",
        "        return\n",
        "\n",
        "    def test(self, on = 'test' , max_outputs = np.inf ):\n",
        "        print('Version ',self.version)\n",
        "        self.test_inputs,self.test_targets,self.test_outputs=self._run_session(mode=on,save_outputs=True,max_output=max_outputs) #stored in RAM\n",
        "        return\n",
        "\n",
        "    def _run_session(self,mode=train, save_outputs=False , max_output = np.inf):\n",
        "        time_batch = 0\n",
        "        time_model = 0\n",
        "        time_loss = 0\n",
        "        time_backprop = 0\n",
        "        if mode == 'train':\n",
        "            self.model.train()\n",
        "            torch.set_grad_enabled(True)\n",
        "            loader=self.train_loader\n",
        "            dict_losses= self.train_losses           \n",
        "        elif mode == 'val':\n",
        "            self.model.eval()\n",
        "            torch.set_grad_enabled(False)\n",
        "            loader = self.val_loader\n",
        "            dict_losses= self.val_losses\n",
        "        elif mode == 'test':\n",
        "            self.model.eval()\n",
        "            torch.set_grad_enabled(False)\n",
        "            loader = self.test_loader\n",
        "            dict_losses = self.test_losses\n",
        "        else:\n",
        "            raise ValueError('mode options are \"train\", \"val\", \"test\" ')\n",
        "        if save_outputs:\n",
        "          test_inputs,test_targets,test_outputs= [], [], {}\n",
        "\n",
        "        len_sess = len(loader.dataset)\n",
        "        epoch_loss= {loss:0 for loss in  self.losses}\n",
        "        iterable=tqdm(enumerate(loader),total=len(loader))\n",
        "        start = time.time()\n",
        "        for batch_idx, (inputs, targets) in iterable:\n",
        "            if self.converge == 0:\n",
        "              return \n",
        "            inputs, targets = self.to_recursive([inputs,targets],device)\n",
        "            end = time.time()\n",
        "            aux = self.auxiliary_inputs(inputs,targets)\n",
        "            time_batch+=end-start          \n",
        "            start = time.time()\n",
        "            outputs =  self.model(inputs,**aux)\n",
        "            end=time.time()\n",
        "            time_model = end-start\n",
        "            batch_loss = self.loss(outputs, inputs, targets)\n",
        "            start = time.time()\n",
        "            for loss in self.losses:\n",
        "               epoch_loss[loss]+=batch_loss[loss].item()\n",
        "            end = time.time()\n",
        "            time_loss += end-start\n",
        "            start = time.time()\n",
        "            if  mode == 'train':\n",
        "              lss = batch_loss['final_loss']\n",
        "              if torch.isinf(lss) or torch.isnan(lss):\n",
        "                self.converge=0\n",
        "              lss.backward()\n",
        "              self.optimizer.step()\n",
        "              self.optimizer.zero_grad()\n",
        "            if save_outputs and max_output > (batch_idx+1)*loader.batch_size:\n",
        "              self.extend_dict_list(test_outputs, self.to_recursive(outputs,'detach_cpu'))\n",
        "              test_inputs.extend(self.to_recursive(inputs,'detach_cpu'))\n",
        "              test_targets.extend(self.to_recursive(targets,'detach_cpu'))\n",
        "            if batch_idx % (len(loader)//10) == 0 and mode=='train':\n",
        "                iterable.set_description('Train [{:4d}/{:4d} ]\\tLoss {:4f}'.format(\n",
        "                     batch_idx * loader.batch_size, len_sess,batch_loss['final_loss'].item()))\n",
        "            if batch_idx == len(loader)-1 and mode=='train':\n",
        "                iterable.set_description('') \n",
        "            end=time.time()\n",
        "            time_backprop += end-start\n",
        "            start = time.time()\n",
        "\n",
        "        end = time.time()\n",
        "        #print('batch, model, loss time, back prop time, recon_time, L1 time, kld time:',time_batch,time_model,time_loss,time_backprop)\n",
        "        num_batch = batch_idx+1\n",
        "        for loss in self.losses:\n",
        "            dict_losses[loss].append(epoch_loss[loss]/ num_batch)\n",
        "        print('Average {} losses :'.format(mode))\n",
        "        for loss in self.losses:\n",
        "            print('{}: {:.4f}'.format(loss,dict_losses[loss][-1]), end='\\t')\n",
        "        print()\n",
        "        if save_outputs:\n",
        "          return  test_inputs,test_targets,test_outputs\n",
        "        else:\n",
        "          return\n",
        "        \n",
        "    def auxiliary_inputs(self,inputs,labels):\n",
        "        return {}\n",
        "\n",
        "    def update_learning_rate(self,new_lr):\n",
        "        for g in self.optimizer.param_groups:\n",
        "            g['lr'] = new_lr\n",
        "\n",
        "    def load(self, epoch=None):\n",
        "        directory=self.version\n",
        "        if epoch:\n",
        "          self.epoch = epoch\n",
        "        else:\n",
        "          past_epochs = []\n",
        "          for file in minioClient.list_objects(self.bin,recursive=True):\n",
        "              file_dir, *file_name = file.object_name.split(\"/\")\n",
        "              if file_dir == directory and file_name[0][:5]=='model':\n",
        "                past_epochs.append(int(re.sub(\"\\D\", \"\",file_name[0])))\n",
        "          if len(past_epochs) == 0:\n",
        "            print(\"No saved models!\")\n",
        "            return\n",
        "          max_epoch = max(past_epochs)\n",
        "          self.epoch = max_epoch\n",
        "        paths = self.paths()\n",
        "        for file in paths.values():\n",
        "            minioClient.fget_object(self.bin,file,os.path.join(self.data_path,file))\n",
        "        self.model.load_state_dict(torch.load(os.path.join(self.data_path,paths['model']),map_location=torch.device(self.device)))        \n",
        "        self.optimizer.load_state_dict(torch.load(os.path.join(self.data_path,paths['optim']),map_location=torch.device(self.device)))\n",
        "        self.train_losses = json.load(open(os.path.join(self.data_path,paths['train_hist'])))\n",
        "        self.val_losses = json.load(open(os.path.join(self.data_path,paths['val_hist'])))\n",
        "\n",
        "        print(\"Loaded: \",paths['model'])\n",
        "        return\n",
        "\n",
        "    def save(self,new_version=None):\n",
        "        self.model.eval()\n",
        "        paths=self.paths(new_version)\n",
        "        torch.save(self.model.state_dict(),os.path.join(self.data_path,paths['model']))\n",
        "        torch.save(self.optimizer.state_dict(),os.path.join(self.data_path,paths['optim']))\n",
        "        json.dump(self.train_losses, open(os.path.join(self.data_path,paths['train_hist']), 'w'))\n",
        "        json.dump(self.val_losses, open(os.path.join(self.data_path,paths['val_hist']), 'w'))\n",
        "        json.dump(self.settings, open(os.path.join(self.data_path,paths['settings']), 'w')) #normally it should be always the same        \n",
        "        for file in paths.values():\n",
        "          minioClient.fput_object(self.bin,file,os.path.join(self.data_path,file))\n",
        "        return\n",
        " \n",
        "    def paths(self,new_version=None):\n",
        "        if new_version:\n",
        "          directory = new_version\n",
        "        else:\n",
        "          directory = self.version \n",
        "        id = self.epoch\n",
        "        try:\n",
        "            os.mkdir(os.path.join(self.data_path,directory))\n",
        "        except:\n",
        "            pass\n",
        "        paths = {}\n",
        "        paths['model'] = \"{}/model_epoch{}.pt\".format(directory,id)\n",
        "        paths['optim'] = \"{}/optimizer_epoch{}.pt\".format(directory,id)\n",
        "        paths['train_hist'] = \"{}/train_losses.json\".format(directory)\n",
        "        paths['val_hist'] = \"{}/val_losses.json\".format(directory)\n",
        "        paths['settings'] = \"{}/settings.json\".format(directory)\n",
        "        return paths\n",
        "\n",
        "    def plot_losses(self):\n",
        "        epochs=np.arange(self.epoch)\n",
        "        plt.plot(epochs, self.train_losses['final_loss'])\n",
        "        if self.val_loader:\n",
        "          plt.plot(epochs, self.val_losses['final_loss'])\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def to_recursive(obj, device): #changes device in dictionary and lists of tensors\n",
        "      if isinstance(obj, list):\n",
        "        obj = [Trainer.to_recursive(item, device) for item in obj]\n",
        "      elif isinstance(obj, dict):\n",
        "        obj = {k: Trainer.to_recursive(v, device) for k,v in obj.items()}\n",
        "      else:\n",
        "        try: \n",
        "          obj=obj.detach().cpu() if device == 'detach_cpu' else  obj.to(device)\n",
        "        except AttributeError:\n",
        "          raise ValueError('Datatype {} do not contain tensors'.format(type(obj)))\n",
        "      return obj\n",
        "\n",
        "    @staticmethod #extends lists in dictionaries\n",
        "    def extend_dict_list(old_dict, new_dict):\n",
        "      for key,value in new_dict.items():         \n",
        "          if key not in old_dict.keys():\n",
        "            old_dict[key]=[]\n",
        "            if isinstance(value,list):\n",
        "              for elem in value:\n",
        "                old_dict[key].append([])\n",
        "          if isinstance(value,list):\n",
        "            for elem, new_elem in zip( old_dict[key],value):\n",
        "              elem.extend(new_elem)\n",
        "          else: \n",
        "            old_dict[key].extend(value)\n",
        "    \n",
        "    @staticmethod #indexes a list (inside of a list) inside of a dictionary\n",
        "    def index_dict_list(dict_list,ind):\n",
        "      list_dict = {}\n",
        "      for k, v in dict_list.items():\n",
        "        if len(v) == 0 or isinstance(v[0],list):\n",
        "          new_v = []\n",
        "          for elem in v:\n",
        "            new_v.append(elem[ind].unsqueeze(0))\n",
        "        else:\n",
        "          new_v = v[ind].unsqueeze(0)\n",
        "        list_dict[k] = new_v\n",
        "      return list_dict\n",
        "\n",
        "    @abstractmethod\n",
        "    def loss(self,output, inputs, targets):\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Q5RlRz_1CCQP"
      },
      "source": [
        "#@title Losses and Visualization\n",
        "\n",
        "class CELoss():\n",
        "    losses=['final_loss','CE loss']\n",
        "    def loss(self, outputs, img, targets):\n",
        "      CE=F.cross_entropy(outputs['probs'], targets)\n",
        "      return {'final_loss':CE,\n",
        "              'CE loss':CE}\n",
        "\n",
        "class DiscriminatorLoss():\n",
        "    losses=['final_loss','True loss','Fake loss']\n",
        "    def loss(self, outputs, img, targets):\n",
        "      true, fake = outputs['wasserstein'].chunk(2,0)\n",
        "      true_loss = true.mean()\n",
        "      fake_loss = -fake.mean()\n",
        "      return {'final_loss':true_loss+fake_loss,\n",
        "              'True loss':true_loss,\n",
        "              'Fake loss':fake_loss}\n",
        "\n",
        "\n",
        "class VAELoss():\n",
        "    losses=['final_loss','real_loss','NLL','MSE','KLD','ADV']    \n",
        "    num_classes = 2\n",
        "    dim_top_channel = (2**(6 - n_resolutions) + 1)**2 #spatial dimension top latent variable\n",
        "    c_KLD = 100\n",
        "    train_var = None #calculated variance\n",
        "    train_logvar = None #calculated logvariance\n",
        "    adj_nll = 0.5*np.log(2*np.pi) #adjustment to calculate the NLL\n",
        "    scale_prior = 5\n",
        "    def loss(self, outputs,inputs,targets):\n",
        "        KLD,KLD_real = self.kld_loss(inputs, outputs, targets)\n",
        "        NLL,MSE = self.nll_loss(inputs, outputs['recon'])\n",
        "        ADV = self.adversarial_loss(inputs,outputs['recon'])\n",
        "        return {'final_loss': NLL + self.c_KLD*KLD + ADV,\n",
        "                'real_loss':NLL + KLD_real,\n",
        "                'KLD': KLD_real,\n",
        "                'NLL': NLL,\n",
        "                'MSE' : MSE,\n",
        "                'ADV': ADV,\n",
        "                }\n",
        "\n",
        "    def nll_loss(self, inputs, recon):\n",
        "         SE = F.mse_loss(recon, inputs, reduction='none')\n",
        "         if self.train_var is None:\n",
        "           self.train_var = SE.mean(0).detach()\n",
        "           self.train_logvar = torch.log(self.train_var)\n",
        "         local_device = recon.device #makes sure you can change device during training\n",
        "         train_var, train_logvar = self.train_var.to(local_device), self.train_logvar.to(local_device)\n",
        "         NLL = 0.5*(SE/train_var + train_logvar).mean(0).sum() + self.adj_nll*train_var.numel()\n",
        "         if self.model.training:\n",
        "           self.train_var = 0.9*train_var+0.1*SE.mean(0).detach()\n",
        "           self.train_logvar = torch.log(self.train_var)\n",
        "         return NLL, SE.mean()\n",
        "    \n",
        "    def kld_loss(self,inputs,outputs,targets,freebits=1):\n",
        "          prior_mu = self.condition(inputs,outputs,targets)\n",
        "          q_mu = outputs['mu'][0]\n",
        "          q_logvar = outputs['log_var'][0]\n",
        "          KLD_matrix = -1 - q_logvar + (q_mu-prior_mu)**2 + q_logvar.exp()\n",
        "          KLD_free_bits = F.softplus(KLD_matrix-2*freebits)+2*freebits\n",
        "          KLD = 0.5 * KLD_free_bits.mean(0).sum()\n",
        "          KLD_real = 0.5 * KLD_matrix.mean(0).sum()\n",
        "          for d_mu, d_logvar, p_logvar in zip(outputs['mu'][1:],outputs['log_var'][1:],outputs['prior_log_var']):\n",
        "                #d_mu = q_mu - p_mu\n",
        "                #d_logvar = q_logvar - p_logvar  \n",
        "                KLD_matrix = -1-d_logvar + d_logvar.exp() + (d_mu**2)/p_logvar.exp()\n",
        "                KLD_free_bits = F.softplus(KLD_matrix-2*freebits)+2*freebits\n",
        "                KLD += 0.5*KLD_free_bits.mean(0).sum()\n",
        "                KLD_real += 0.5*KLD_matrix.mean(0).sum()\n",
        "          return KLD,KLD_real\n",
        "\n",
        "    def condition(self,inputs,outputs,targets):\n",
        "          return torch.zeros_like(outputs['mu'][0])\n",
        "\n",
        "    def adversarial_loss(self,inputs,recon):\n",
        "          return torch.zeros(1).to(self.device)\n",
        "        \n",
        "        \n",
        "class CVAELossPrior(VAELoss):\n",
        "    # prior condition on targets\n",
        "    def condition(self,inputs,outputs,targets):      \n",
        "      conditional = torch.zeros_like(outputs['mu'][0])\n",
        "      for t, target in enumerate(targets):\n",
        "        if target != self.num_classes - 1:\n",
        "          conditional[t,target*self.dim_top_channel:(target+1)*self.dim_top_channel] = self.scale_prior\n",
        "      return conditional\n",
        "\n",
        "\n",
        "class VAELossPrior(VAELoss):\n",
        "    #prior condition on probablities (calculated for simplicity, can be stored)\n",
        "    def condition(self,inputs,outputs,targets):\n",
        "      f = lambda x: (torch.sqrt(x)-torch.sqrt(1-x)+1)/2\n",
        "      with torch.no_grad():\n",
        "        #these loss is inherited to an object with an attribute classifier\n",
        "        self.classifier.eval()\n",
        "        probs = self.classifier(inputs.to(self.device))['probs']\n",
        "        probs = f(f(torch.softmax(probs, dim=1)[:,:-1]))\n",
        "      conditional = torch.zeros_like(outputs['mu'][0])\n",
        "      conditional[:,:self.dim_top_channel*(self.num_classes-1)]= probs.repeat_interleave(self.dim_top_channel,1)*self.scale_prior\n",
        "      return conditional\n",
        "\n",
        "\n",
        "class VAEXLossPrior(VAELoss):\n",
        "    #prior condition on stored probabilities\n",
        "    def condition(self,inputs,outputs,targets):\n",
        "      probs=outputs['condition']\n",
        "      conditional = torch.zeros_like(outputs['mu'][0])\n",
        "      conditional[:,:self.dim_top_channel*(self.num_classes-1)]=probs*self.scale_prior\n",
        "      return conditional\n",
        "\n",
        "class Explain_VAE():\n",
        "    #some attributes are found in the loss and in the trainer classes    \n",
        "    label_names = ['Female','Male']\n",
        "    def visualize_cvae(self,ind,n_delta=5):\n",
        "      if not self.test_outputs:\n",
        "        print('Test data have not been gathered. Use test() method first.')\n",
        "        return\n",
        "      self.classifier.eval()\n",
        "      self.model.eval()\n",
        "      img = self.test_inputs[ind].to(self.device).unsqueeze(0)\n",
        "      target = self.test_targets[ind].to(self.device).unsqueeze(0)\n",
        "      print('True label: {}'.format(target.item()))\n",
        "      outputs = self.index_dict_list(self.test_outputs,ind=ind)\n",
        "      outputs = self.to_recursive(outputs, self.device)\n",
        "      loss = self.loss(outputs, img, target)\n",
        "      print('Recon MSE:{:.2f}% KLD:{:.2f}'.format(loss['MSE']*100,loss['KLD']))\n",
        "      if 'condition' in outputs.keys():\n",
        "        condition = {'condition':torch.Tensor([[outputs['condition']]]).to(device)}\n",
        "      else:\n",
        "        condition = {}\n",
        "      with torch.no_grad():\n",
        "        self.classifier.eval()\n",
        "        self.model.eval()\n",
        "        data = self.model.encoder(img,**condition)\n",
        "        original_cut = self.model.decoder(data,s=0,**condition)\n",
        "        original_prob, recon_prob, cut_prob = torch.softmax(self.classifier(torch.cat([img,outputs['recon'],original_cut['recon']]))['probs'], dim = 1)\n",
        "      original_pred = torch.argmax(original_prob)\n",
        "      \n",
        "      plt.figure(figsize=(25,10))\n",
        "      plt.subplot(1, 3, 1)\n",
        "      label_name = self.label_names[target]\n",
        "      plt.title('Original ({}: {:.2f})'.format(label_name,original_prob[target].item()), fontsize=20)\n",
        "      plt.imshow(self.show(img.squeeze()) , cmap='gray')\n",
        "      plt.axis('off')\n",
        "      plt.subplot(1, 3, 2)\n",
        "      plt.title('Reconstruction ({}: {:.2f})'.format(label_name,recon_prob[target].item()), fontsize=20)\n",
        "      plt.imshow(self.show(outputs['recon'].squeeze()), cmap='gray')\n",
        "      plt.axis('off')  \n",
        "      plt.subplot(1, 3, 3)\n",
        "      plt.title('Simple cut ({}: {:.2f})'.format(label_name,cut_prob[target].item()), fontsize=20)\n",
        "      plt.imshow(self.show(original_cut['recon'].squeeze()), cmap='gray')\n",
        "      plt.axis('off')\n",
        "      plt.show()    \n",
        "      if self.classifier is None: #for model without conditioning\n",
        "        return\n",
        "      counterfactuals = []\n",
        "      counterfactuals_prob = []\n",
        "      for target in range(self.num_classes):\n",
        "        counterfactuals_target,counterfactuals_prob_target = self.do_condition(img,target,n_delta)\n",
        "        counterfactuals.append(counterfactuals_target)\n",
        "        counterfactuals_prob.append(counterfactuals_prob_target)\n",
        "        plt.figure(figsize=(25,10))\n",
        "        for i in range(n_delta):\n",
        "            plt.subplot(1, n_delta, i+1)\n",
        "            plt.title('r =  {:.2f}: ({}:{:.2f})'.format(1-i/n_delta,self.label_names[target], counterfactuals_prob[target][i][target].item()), fontsize=18)\n",
        "            plt.imshow(self.show(counterfactuals[target][i].squeeze()), cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.show()\n",
        "      \n",
        "    def do_condition(self,inputs,target,n_delta=6):\n",
        "      condition = Trainer.to_recursive(self.condition_input(target),self.device)\n",
        "      with torch.no_grad():\n",
        "        self.model.eval()\n",
        "        data = self.model.encoder(inputs,**condition)\n",
        "        data['z'][-1] = self.condition_latent(data['z'][-1],target).to(self.device)\n",
        "        counterfactuals_target = []\n",
        "        counterfactuals_prob_target = []\n",
        "        for s in range(n_delta,0,-1):\n",
        "          data_copy = data.copy()\n",
        "          data_copy['z'] = data['z'].copy()\n",
        "          data_copy['hidden'] = data['hidden'].copy()\n",
        "          s/=n_delta\n",
        "          counterfactual = self.model.decoder(data_copy, s=s,**condition)['recon']\n",
        "          counterfactuals_target += [counterfactual]\n",
        "          counterfactuals_prob_target += [torch.softmax(self.classifier(counterfactual)['probs'],dim=1).squeeze()]\n",
        "      return counterfactuals_target, counterfactuals_prob_target\n",
        "\n",
        "    def condition_latent(self,z,target):\n",
        "      return z\n",
        "\n",
        "    def condition_input(self,target):\n",
        "      return {}\n",
        "\n",
        "    def anomaly_detection(self, wrong_indices):\n",
        "      if not self.test_outputs:\n",
        "        print('Test data have not been gathered. Use test() method first.')\n",
        "        return\n",
        "      n_misclassified = len(wrong_indices)\n",
        "      if not n_misclassified:\n",
        "        print('No misclassified samples')\n",
        "        return\n",
        "      misclassified_img = torch.zeros(n_misclassified,*self.test_inputs[0].size())\n",
        "      misclassified_targets = torch.LongTensor(n_misclassified,*self.test_targets[0].size()).zero_()\n",
        "      for i, index in enumerate(wrong_indices):\n",
        "        misclassified_img[i] = self.test_inputs[index]\n",
        "        misclassified_targets[i] = self.test_targets[index].long()\n",
        "      #creates a dictionary of tensors with N = n_misclassified\n",
        "      misclassified_outputs = {}\n",
        "      for k, v in self.test_outputs.items():\n",
        "        if not len(v):\n",
        "          continue\n",
        "        if isinstance(v[0],list):\n",
        "          if k not in misclassified_outputs.keys():\n",
        "            misclassified_outputs[k]=[]   \n",
        "          for tensor_list in v:\n",
        "            to_concat = []            \n",
        "            for index in wrong_indices:\n",
        "              to_concat.append(tensor_list[index].unsqueeze(0))\n",
        "            misclassified_outputs[k].append(torch.cat(to_concat))\n",
        "        else:\n",
        "          to_concat = []            \n",
        "          for index in wrong_indices:\n",
        "            to_concat.append(v[index].unsqueeze(0))\n",
        "          misclassified_outputs[k] = torch.cat(to_concat)\n",
        "      print(misclassified_targets.dtype)\n",
        "      misclassified_loss = self.loss(misclassified_outputs,misclassified_img,misclassified_targets)\n",
        "      print('Misclassified loss')\n",
        "      for loss, value in misclassified_loss.items():\n",
        "        print('{}: {:.4f}'.format(loss,value.item()), end='\\t')\n",
        "      print()    \n",
        "      \n",
        "    @staticmethod\n",
        "    def show(tensor):\n",
        "        return tensor.detach().cpu().numpy().transpose(1,2,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "O_6Ban49CCQQ"
      },
      "source": [
        "#@title Utils, Layers and Blocks\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self, h_in):\n",
        "        super().__init__()\n",
        "    def forward(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "#increased momentum\n",
        "class WobblyBatchNorm(nn.Module):\n",
        "    def __init__(self, h_in):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(h_in,eps=1e-6,momentum=0.2)\n",
        "    def forward(self, inputs):\n",
        "        return self.bn(inputs)\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, x):\n",
        "        return x*torch.sigmoid(x)\n",
        "\n",
        "\n",
        "BatchAct = nn.Hardswish #more efficient than Swhsh\n",
        "BatchNorm = WobblyBatchNorm\n",
        "wn = torch.nn.utils.weight_norm\n",
        "\n",
        "#creates new version of a dictionary\n",
        "class FunctionalDict(dict):\n",
        "    def but_with(self, **args):\n",
        "      new_dict = self.copy()\n",
        "      new_dict.update(**args)\n",
        "      return new_dict\n",
        "\n",
        "#more elegant than zip(reverse(),reverse(),..)\n",
        "def reverse_zip(*args):\n",
        "  rev_args = []\n",
        "  for arg in args:\n",
        "    rev_args.append(arg[::-1]) \n",
        "  return zip(*rev_args)\n",
        "\n",
        "\n",
        "class View(nn.Module):\n",
        "\n",
        "    def __init__(self, *shape):\n",
        "        super().__init__()\n",
        "        self.shape = shape\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.view(self.shape)\n",
        "        \n",
        "class DebugLayer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        print(input.size())\n",
        "        return input\n",
        "\n",
        "class SqueezeExcite(nn.Module):\n",
        "    def __init__(self, h_dim,reduction_ratio=0.1):\n",
        "        super().__init__()\n",
        "        h_reduced= max(1,int(h_dim*reduction_ratio))\n",
        "        self.fcreduce = nn.Linear(h_dim, h_reduced)\n",
        "        self.fcaugment = nn.Linear(h_reduced, h_dim)\n",
        "    def forward(self, inputs):\n",
        "        #global average pooling\n",
        "        x = inputs.mean((2,3))\n",
        "        x = F.relu(self.fcreduce(x))       \n",
        "        x = torch.sigmoid(self.fcaugment(x))\n",
        "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
        "        return inputs.mul(x) \n",
        "\n",
        "class MobileBlock(nn.Module):\n",
        "    def __init__(self, h_in, h_out, deconv=False, kernel_size=5, stride=1, padding=2, expansion_factor=4, Act=BatchAct):\n",
        "        super().__init__()\n",
        "        h_exp=h_in*expansion_factor\n",
        "        self.stride = stride\n",
        "        Conv = nn.ConvTranspose2d if deconv else  nn.Conv2d\n",
        "        self.block= nn.Sequential(BatchNorm(h_in),\n",
        "                                  Act(),\n",
        "                                  nn.Conv2d(h_in, h_exp, 1,1, bias = False),\n",
        "                                  BatchNorm(h_exp),\n",
        "                                  Act(),            \n",
        "                                  Conv(h_exp, h_exp, kernel_size=kernel_size, stride=stride, padding=padding, groups=h_exp, bias = False), \n",
        "                                  BatchNorm(h_exp),\n",
        "                                  Act(),\n",
        "                                  nn.Conv2d(h_exp, h_out, 1,1, bias = False),\n",
        "                                  BatchNorm(h_out),\n",
        "                                  )\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.block(x)\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, h_in, h_out, deconv=False, kernel_size=3, stride=1, padding=1, Act=BatchAct, wn = lambda x:x):\n",
        "      super().__init__()\n",
        "      self.stride = stride\n",
        "      Conv = nn.ConvTranspose2d if deconv else  nn.Conv2d\n",
        "      self.block= nn.Sequential(  BatchNorm(h_in),\n",
        "                                  Act(),\n",
        "                                  wn(Conv(h_in, h_out, kernel_size=kernel_size, stride=stride, padding=padding, bias = False)), \n",
        "                                  )\n",
        "      \n",
        "    def forward(self, x):\n",
        "      return self.block(x)\n",
        "\n",
        "class ResLayer(nn.Module):\n",
        "    def __init__(self, Block, h_in):\n",
        "        super().__init__()  \n",
        "        self.block=Block(h_in,h_in)\n",
        "        self.se = SqueezeExcite(h_in)\n",
        "    def forward(self, x):\n",
        "      return x + self.se(self.block(x))\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, Block, h_in, h_out, kernel_size, stride, padding, deconv=False, depth=0, interpolate=False):\n",
        "        super().__init__()\n",
        "        modules=[]\n",
        "        self.stride = stride\n",
        "        self.deconv = deconv\n",
        "        self.interpolate = interpolate\n",
        "        self.h_in = h_in\n",
        "        self.h_out = h_out\n",
        "        if interpolate:\n",
        "          self.se = SqueezeExcite(h_out)\n",
        "        for _ in range(depth):\n",
        "          modules.append(ResLayer(Block, h_in))\n",
        "        modules.append(Block(h_in, h_out, deconv=deconv, kernel_size=kernel_size, stride=stride, padding=padding))\n",
        "        self.outer_block=nn.Sequential(*modules)\n",
        "    def forward(self, x):\n",
        "      y=self.outer_block(x)\n",
        "      if not self.interpolate:\n",
        "        return y\n",
        "      output_size=y.size()\n",
        "      if self.stride!=1:\n",
        "        x= F.interpolate(x,size=output_size[2:],mode='bilinear',align_corners = False)\n",
        "      if self.h_out < self.h_in: #input channels are twice as much \n",
        "        x=x.view(output_size[0],output_size[1],2,output_size[2],output_size[3]).mean(2)\n",
        "      elif self.h_out > self.h_in: #output channels are twice as much \n",
        "        x=x.repeat(1,2,1,1)\n",
        "      return x + self.se(y)\n",
        "\n",
        "\n",
        "class FromLatentConv(nn.Module):\n",
        "    def __init__(self, h_dim, z_dim, l_dim, block=ConvBlock):\n",
        "      super().__init__()\n",
        "      self.toimage = nn.Sequential(\n",
        "                                  View(-1,z_dim,(l_dim+1)//2,(l_dim+1)//2),\n",
        "                                  block(z_dim,h_dim, stride=2, deconv=True),\n",
        "                                  SqueezeExcite(h_dim),\n",
        "                                  )\n",
        "                           \n",
        "    def forward(self, z, x):\n",
        "      z = self.toimage(z)\n",
        "      return x + z\n",
        "\n",
        "#uses inverted block + concatenation\n",
        "class ToLatentConvMobile(nn.Module):\n",
        "    def __init__(self,h_dim, z_dim, l_dim, aux_dim = 0,block=ConvBlock):\n",
        "        super().__init__()\n",
        "        if aux_dim:\n",
        "          self.aux_conv =  nn.Sequential(\n",
        "                                    View(-1,aux_dim,(l_dim+3)//4,(l_dim+3)//4),\n",
        "                                    MobileBlock(aux_dim,aux_dim, deconv=True, stride=4),\n",
        "                                    SqueezeExcite(aux_dim)\n",
        "                                    )\n",
        "        self.tolatent = block(h_dim + aux_dim, 2*z_dim, stride=2, wn=wn )\n",
        "        \n",
        "    def forward(self, x, aux = None):\n",
        "        if aux is not None:\n",
        "          aux = self.aux_conv(aux)\n",
        "          x = torch.cat([x,aux], dim=1)\n",
        "        x = self.tolatent(x)\n",
        "        return x.flatten(1).chunk(2, 1)\n",
        "\n",
        "#uses ADAIN\n",
        "class ToLatentConvADAIN(nn.Module):\n",
        "    def __init__(self,h_dim, z_dim, l_dim, aux_dim = 0,block=ConvBlock):\n",
        "        super().__init__()\n",
        "        if aux_dim:\n",
        "          self.aux_conv =  nn.Sequential(\n",
        "                                    nn.Linear(aux_dim*((l_dim+3)//4)**2,2*h_dim),\n",
        "                                    View(-1,2*h_dim,1,1),\n",
        "                                    )\n",
        "        self.tolatent = block(h_dim, 2*z_dim, stride=2, wn=wn)\n",
        "        \n",
        "    def forward(self, x, aux = None):\n",
        "        if aux is not None:\n",
        "          mu,std = self.aux_conv(aux).chunk(2,1)\n",
        "          x = F.instance_norm(x)*torch.sigmoid(std)+mu\n",
        "        x = self.tolatent(x)\n",
        "        return x.flatten(1).chunk(2, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPjViLCYCCQQ"
      },
      "source": [
        "### Training Standard classifier on Celeba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "4HJEfA-oCCQR"
      },
      "source": [
        "#@title Classifiers\n",
        "class Celebaclassifier(nn.Module):\n",
        "    num_classes = 2\n",
        "    settings = {}\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        modules = []\n",
        "        in_channels = 3\n",
        "        in_dims = x_dim\n",
        "        for _ in range(5):\n",
        "          in_dims//=2\n",
        "          out_channels=2*in_channels\n",
        "          modules.append(nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, 1, bias = False),\n",
        "                                        nn.AdaptiveMaxPool2d(in_dims),\n",
        "                                        nn.BatchNorm2d(out_channels),\n",
        "                                        nn.LeakyReLU(0.1),\n",
        "                                                               ))\n",
        "          in_channels=out_channels\n",
        "        self.net = nn.Sequential(*modules,\n",
        "                                 nn.Flatten(),\n",
        "                                 nn.Dropout2d(0.5),\n",
        "                                 nn.Linear(in_channels*in_dims**2, self.num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return {'probs': self.net(x)}\n",
        "\n",
        "\n",
        "celeba_classifier = Celebaclassifier()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "mb9d-ypBCCQR"
      },
      "source": [
        "#@title Trainer classifier\n",
        "class ClassifierTrainer(CELoss,Trainer):\n",
        "    def __init__(self, model,version,block_args):\n",
        "        super().__init__(model,version,**block_args)\n",
        "        self.pred = []\n",
        "        self.wrong_indices = []\n",
        "\n",
        "    def test(self, on = 'test', max_outputs = None):\n",
        "      print('Version ',self.version)\n",
        "      super().test(on=on,max_outputs=max_outputs)\n",
        "      self.pred=np.array([torch.softmax(t, dim = 0).numpy().argmax() for t in self.test_outputs['probs']]) #stored predictions\n",
        "      accuracy=(self.pred==self.test_targets).sum()/len(self.pred) #overall accuracy\n",
        "      self.wrong_indices=np.where(self.pred!=self.test_targets)    #mispredicted samples' index\n",
        "      print('Accuracy : {}'.format(accuracy))\n",
        "      return \n",
        "\n",
        "\n",
        "block_args_classifier={\n",
        "    'optim':optim.Adam,\n",
        "    'initial_lr': 1e-3,\n",
        "    'weight_decay':0,\n",
        "    'train_loader':train_loader,\n",
        "    'val_loader': val_loader,\n",
        "    'device':device,\n",
        "    #'test_loader': test_loader\n",
        "}\n",
        "classifier_trainer=ClassifierTrainer(celeba_classifier,'Celeba Classifier',block_args_classifier)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgCRFJUtCCQS"
      },
      "source": [
        "### Training VAE classifier on Celeba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN_OzFQjb8sh",
        "cellView": "form"
      },
      "source": [
        "#@title Saved models\n",
        "for model_name in minioClient.list_objects('celeba'):\n",
        "     name = model_name.object_name\n",
        "     if name[-1] == \"/\":\n",
        "         print(name[:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "VZXLtk31CCQS"
      },
      "source": [
        "#@title VAE version\n",
        "model = 'BVAE'  #@param [\"BVAE\"] at the moment the only option\n",
        "explainability = \"XPrior\" #@param [\"\",\"Prior\",\"C\",\"X\",\"CPrior\",\"XPrior\"]\n",
        "vae_version = ''  #@param {type: \"string\"}\n",
        "discriminator = None\n",
        "vae_versions = {}\n",
        "base_version = FunctionalDict(\n",
        "   x_dim = x_dim,\n",
        "   h_dims = [2**i for i in range(4,4+n_resolutions+1)],\n",
        "   k_dims = [3]*n_resolutions, \n",
        "   strides = [2]*n_resolutions,\n",
        "   paddings = [1]*n_resolutions,\n",
        "   z_dims = [2**i for i in range(n_resolutions+1)],\n",
        "   depths = [1]*n_resolutions,\n",
        "   interpolate = True,\n",
        "   Block = ConvBlock,\n",
        "   Inference = ToLatentConvADAIN \n",
        "   )   \n",
        "\n",
        "hierarchy2 = [2*2**i for i in range(n_resolutions+1)]\n",
        "vae_versions[''] = base_version\n",
        "vae_versions['Mobile'] = base_version.but_with(Inference=ToLatentConvMobile)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GXm9xF99CCQT"
      },
      "source": [
        "#@title VAE models\n",
        "class Abstract_VAE(nn.Module,metaclass=ABCMeta):\n",
        "    in_channels = 3\n",
        "    out_channel = 3\n",
        "    dim_target = 0\n",
        "\n",
        "    def __init__(self, settings):\n",
        "        super().__init__()\n",
        "        # Dimensions\n",
        "        self.x_dim = settings['x_dim']\n",
        "        self.h_dims = settings['h_dims']\n",
        "        self.k_dims = settings['k_dims']\n",
        "        self.strides = settings['strides']\n",
        "        self.paddings = settings['paddings']\n",
        "        self.z_dims = settings['z_dims']\n",
        "        self.l_dims_encoder, self.l_dims_decoder, self.flat_dim = self.size_calc()\n",
        "\n",
        "\n",
        "    def forward(self, x, **condition):\n",
        "        inference_data = self.encoder(x, **condition) \n",
        "        return self.decoder(inference_data, **condition)\n",
        "\n",
        "    def sampling(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = eps.mul(std).add_(mu) if self.training else eps.mul(std/3).add_(mu)\n",
        "        return z\n",
        "    \n",
        "    @abstractmethod\n",
        "    def encoder(self, x):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def decoder(self, x):\n",
        "        pass\n",
        "    \n",
        "    def print_total_parameters(self):\n",
        "        num_params = 0\n",
        "        for param in self.parameters():\n",
        "            num_params += param.numel()\n",
        "        print('Total Parameters: {}'.format(num_params))\n",
        "        return \n",
        "\n",
        "    def size_calc(self):\n",
        "        l_dims = self.x_dim\n",
        "        l_dims_encoder = [l_dims]\n",
        "        for k_dim, stride, padding in zip(self.k_dims, self.strides, self.paddings):\n",
        "            l_dims = self.conv_size_calc(L_in=l_dims, kernel_size=k_dim,stride=stride,padding=padding)\n",
        "            l_dims_encoder.append(l_dims)\n",
        "        flat_dim = self.h_dims[-1] * l_dims_encoder[-1] ** 2\n",
        "        l_dims_decoder = [l_dims]\n",
        "        for k_dim, stride, padding in reverse_zip(self.k_dims, self.strides, self.paddings):\n",
        "            l_dims= self.convt_size_calc(L_in=l_dims, kernel_size=k_dim,stride=stride,padding=padding)\n",
        "            l_dims_decoder.append(l_dims)\n",
        "        assert l_dims == self.x_dim\n",
        "        return l_dims_encoder, l_dims_decoder, flat_dim\n",
        "\n",
        "    def print_sizes(self):\n",
        "        print('Block Sizes:')\n",
        "        for i, (h, l) in enumerate(zip( self.h_dims, self.l_dims_encoder )):\n",
        "            print('Block {} size: Nonex{}x{}x{}'.format(i + 1, h, l, l))\n",
        "\n",
        "        for i, (h, l) in enumerate(zip( self.h_dims[::-1] , self.l_dims_decoder )):\n",
        "            print('Block {} size: Nonex{}x{}x{}'.format(len(self.h_dims) +1 + i , h, l, l))\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def conv_size_calc(L_in, kernel_size, stride=1, dilation=1, padding=0):\n",
        "        return int((L_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride) + 1\n",
        "    \n",
        "    @staticmethod\n",
        "    def convt_size_calc(L_in, kernel_size, stride=1, padding=0, dilation=1, output_padding=0):\n",
        "        return (L_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
        "\n",
        "\n",
        "class BVAE(Abstract_VAE):\n",
        "\n",
        "    def __init__(self, Block, Inference, **other_settings):\n",
        "        super().__init__(other_settings) #calculates final layer. \n",
        "        \n",
        "        # settings will be saved\n",
        "        h_dims = other_settings['h_dims']\n",
        "        k_dims = other_settings['k_dims']\n",
        "        strides = other_settings['strides']\n",
        "        paddings = other_settings['paddings']\n",
        "        *c_z_dims, final_z_dim = other_settings['z_dims']\n",
        "        depths = other_settings['depths']\n",
        "        interpolate = other_settings['interpolate']\n",
        "        self.settings = {**other_settings, 'Block':str(Block), 'Inference':str(Inference)}\n",
        "\n",
        "        # encoder\n",
        "        self.conv_init = nn.Conv2d(self.in_channels, h_dims[0], 1, bias = False)\n",
        "        self.encoding = nn.ModuleList([])\n",
        "        *resolutions, final_res = self.l_dims_encoder\n",
        "        aux_dim = final_z_dim + (self.dim_target>0)\n",
        "        for i, (k_dim, stride, padding, z_dim, depth) in enumerate(zip(k_dims, strides, paddings, c_z_dims, depths)):\n",
        "          encode = ResBlock(Block,h_dims[i], h_dims[i+1], k_dim, stride, padding, deconv=False, depth=depth, interpolate=interpolate)\n",
        "          self.encoding.append(encode) \n",
        "\n",
        "        self.final_inference = Inference( h_dims[-1], final_z_dim, final_res, 0)\n",
        "        self.h = torch.nn.parameter.Parameter(torch.ones(1,h_dims[-1],final_res, final_res))\n",
        "        self.back_to_img = FromLatentConv(h_dims[-1], aux_dim,  final_res)\n",
        "\n",
        "        # decoder\n",
        "        self.inference = nn.ModuleList([])\n",
        "        self.decoding = nn.ModuleList([])\n",
        "        self.generate = nn.ModuleList([])\n",
        "        self.from_latent = nn.ModuleList([])\n",
        "        self.merge = nn.ModuleList([])\n",
        "        for i, (k_dim, stride, padding, z_dim, res, depth) in enumerate(reverse_zip(k_dims, strides, paddings, c_z_dims, resolutions, depths)):\n",
        "          decode = ResBlock(Block,h_dims[-i-1], h_dims[-i-2], k_dim, stride, padding, deconv=True, depth=depth, interpolate=interpolate)\n",
        "          from_latent = FromLatentConv(h_dims[-i-2], z_dim,  res) if z_dim else None\n",
        "          generate = Inference(h_dims[-i-2], z_dim, res, aux_dim) if z_dim else None\n",
        "          merge =  Block(2*h_dims[-i-2], h_dims[-i-2]) if z_dim else None\n",
        "          inference = Inference(h_dims[-i-2], z_dim, res) if z_dim else None\n",
        "          self.decoding.append(decode)\n",
        "          self.inference.append(inference)\n",
        "          self.generate.append(generate)\n",
        "          self.from_latent.append(from_latent)\n",
        "          self.merge.append(merge)  \n",
        "          aux_dim = z_dim or 0          \n",
        "        self.conv_final= Block(h_dims[0], self.out_channel)\n",
        "\n",
        "    def encoder(self, x):\n",
        "        data={'hidden':[],\n",
        "              'mu':[],\n",
        "              'log_var':[],\n",
        "              'z':[]}\n",
        "        x = self.conv_init(x)\n",
        "        for encode in self.encoding:\n",
        "          data['hidden'].append(x)\n",
        "          x=encode(x)\n",
        "\n",
        "        mu, log_var = self.final_inference(x)\n",
        "        z = self.sampling(mu, log_var)\n",
        "        \n",
        "        data['mu'].append(mu)\n",
        "        data['log_var'].append(log_var)        \n",
        "        data['z'].append(z)          \n",
        "        return data\n",
        "       \n",
        "    def decoder(self, data, sample=None, s=1):\n",
        "        data['prior_mu']=[]\n",
        "        data['prior_log_var']=[]\n",
        "        z = data['z'].pop() if sample is None else sample\n",
        "        h = self.h.expand(z.size()[0],-1,-1,-1)\n",
        "        x=self.back_to_img(z, h)\n",
        "        for decode, from_latent, generate, inference, merge in zip(self.decoding, self.from_latent, self.generate, self.inference, self.merge):\n",
        "          x = decode(x)\n",
        "          h = data['hidden'].pop() if sample is None else None\n",
        "          data['hidden'].insert(0,h)\n",
        "          if from_latent:\n",
        "            p_mu,p_logvar = generate(x, z)\n",
        "            if sample is None:\n",
        "              h = merge(torch.cat([x,h], dim=1))\n",
        "              mu, log_var = inference(h)\n",
        "              z = self.sampling(s*mu+p_mu, log_var+p_logvar)\n",
        "              data['mu'].append(mu)\n",
        "              data['log_var'].append(log_var)\n",
        "            else:\n",
        "              z = self.sampling(p_mu, p_logvar)\n",
        "            x = from_latent(z,x)\n",
        "            data['prior_mu'].append(p_mu)\n",
        "            data['prior_log_var'].append(p_logvar)\n",
        "            s *= s            \n",
        "        data['recon'] = torch.sigmoid(self.conv_final(x))\n",
        "        return data\n",
        "\n",
        "\n",
        "class VAEX():\n",
        "\n",
        "    in_channels=4\n",
        "    dim_target=1\n",
        "\n",
        "    def __init__(self, Act=nn.ELU, **settings):\n",
        "        super().__init__(**settings)\n",
        "        final_res = (self.l_dims_encoder[-1]+1)//2\n",
        "        self.fc0 = nn.Sequential(nn.Linear( self.dim_target, x_dim**2), Act(),  View(-1, 1 ,x_dim, x_dim))\n",
        "        self.fc1 = nn.Sequential(nn.Linear( self.dim_target, final_res**2), Act())\n",
        "\n",
        "    def encoder(self, x, condition):\n",
        "        x = torch.cat([x, self.fc0(condition)], dim=1)\n",
        "        return  super().encoder(x)\n",
        "\n",
        "    def decoder(self, data, condition, sample = None, s=1):\n",
        "        data['condition'] = condition\n",
        "        condition = self.fc1(condition)\n",
        "        if sample is None:\n",
        "            x = data['z'][-1]\n",
        "            data['z'][-1] = torch.cat([x, condition], dim=1)\n",
        "        else:\n",
        "            sample = torch.cat([sample, condition], dim=1)\n",
        "        return super().decoder(data, sample, s=s)\n",
        "\n",
        "\n",
        "class Empty: pass\n",
        "\n",
        "\n",
        "settings = vae_versions[vae_version] \n",
        "Model = { 'BVAE' : BVAE }\n",
        "Explainability = {'': Empty,\n",
        "                  \"Prior\": Empty,\n",
        "                  \"C\" : VAEX,\n",
        "                  \"X\" : VAEX,\n",
        "                  \"CPrior\" : Empty,\n",
        "                  \"XPrior\" : VAEX,\n",
        "                  }                  \n",
        "\n",
        "class ModelFinal(Explainability[explainability],Model[model]): pass \n",
        "\n",
        "\n",
        "vae = ModelFinal(**settings)\n",
        "vae.print_total_parameters()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "B0JNyg1_CCQT"
      },
      "source": [
        "#@title Trainer VAE Classes\n",
        "class VAETrainer(VAELoss,Trainer,Explain_VAE):\n",
        "  \n",
        "    def __init__(self, model,version,classifier,discriminator,block_args):\n",
        "        super().__init__(model,version,**block_args)\n",
        "        self.discriminator = discriminator\n",
        "        self.classifier = classifier\n",
        "\n",
        "\n",
        "class CVAETrainer(VAETrainer):\n",
        "\n",
        "    def __init__(self, model,version,classifier,discriminator,block_args):\n",
        "        super().__init__(model,version,classifier,discriminator,block_args)\n",
        "\n",
        "    def auxiliary_inputs(self,img,labels):\n",
        "      condition = torch.zeros_like(labels,dtype=torch.float).unsqueeze(1).repeat(1,self.num_classes-1)\n",
        "      for i, l in enumerate(labels):\n",
        "        if l != self.num_classes-1:\n",
        "          condition[i,l] = 1\n",
        "      return {'condition':condition} #to be fixed for multidimensional labels\n",
        "\n",
        "    def condition_input(self,target):\n",
        "      condition = torch.zeros(self.num_classes-1,dtype=torch.float)\n",
        "      if target != self.num_classes-1:\n",
        "          condition[target] = 1      \n",
        "      return {'condition': condition.unsqueeze(0)}\n",
        "    \n",
        "\n",
        "class CVAEPriorTrainer(CVAELossPrior,VAETrainer):\n",
        "\n",
        "    def __init__(self, model,version,classifier,discriminator,block_args):\n",
        "        super().__init__(model,version,classifier,discriminator,block_args)\n",
        "\n",
        "    def condition_latent(self,z,target):\n",
        "      dim_top_channel = self.dim_top_channel #from the VAELoss class\n",
        "      scale_prior = self.scale_prior #from the VAELoss class\n",
        "      num_classes = self.num_classes\n",
        "      z[:,:dim_top_channel*(num_classes-1)] = 0\n",
        "      if target != num_classes - 1:\n",
        "        z[:,target*dim_top_channel:(target+1)*dim_top_channel] = scale_prior\n",
        "      return z\n",
        "\n",
        "class VAEXTrainer(CVAETrainer):\n",
        "\n",
        "    def __init__(self, model,version,classifier,discriminator,block_args):\n",
        "        super().__init__(model,version,classifier,discriminator,block_args)\n",
        "\n",
        "    def auxiliary_inputs(self,img,labels):\n",
        "      #this is made for pragmatism, the probabilities could be passed on with the batch loader\n",
        "      with torch.no_grad():\n",
        "        self.classifier.eval()\n",
        "        output=self.classifier(img)\n",
        "        #center the probabilities around 1/2\n",
        "        f = lambda x: (torch.sqrt(x)-torch.sqrt(1-x)+1)/2     \n",
        "      return {'condition': f(f(torch.softmax(output['probs'],dim=1)[:,:-1]))}\n",
        "\n",
        "\n",
        "class VAEPriorTrainer(VAELossPrior, CVAEPriorTrainer):\n",
        "\n",
        "    def __init__(self, model,version,classifier,discriminator,block_args):\n",
        "        super().__init__(model,version,classifier,discriminator,block_args)\n",
        "\n",
        "\n",
        "class VAEXPriorTrainer(VAEXLossPrior, VAEXTrainer):\n",
        "\n",
        "    def __init__(self, model,version,classifier,discriminator,block_args):\n",
        "        super().__init__(model,version,classifier,discriminator,block_args)\n",
        "\n",
        "    def condition_latent(self,z,target):\n",
        "      dim_top_channel = self.dim_top_channel #from the VAELoss class\n",
        "      scale_prior = self.scale_prior #from the VAELoss class\n",
        "      num_classes = self.num_classes\n",
        "      z[:,:dim_top_channel*(num_classes-1)] = 0\n",
        "      if target != num_classes - 1:\n",
        "        z[:,target*dim_top_channel:(target+1)*dim_top_channel] = scale_prior\n",
        "      return z\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "akJ9lg6ACCQU"
      },
      "source": [
        "#@title Trainer VAE\n",
        "Trainer_dict = {'': VAETrainer,\n",
        "                \"Prior\": VAEPriorTrainer,\n",
        "                \"C\" : CVAETrainer,\n",
        "                \"X\" : VAEXTrainer,\n",
        "                \"CPrior\" : CVAEPriorTrainer,\n",
        "                \"XPrior\" : VAEXPriorTrainer,\n",
        "                  }    \n",
        "\n",
        "vae_trainer=Trainer_dict[explainability](vae,model+explainability+vae_version+str(''),celeba_classifier, None, block_args)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De1YTXPXCCQU"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "cellView": "form",
        "id": "Gw40_a5zCCQV"
      },
      "source": [
        "#@title Load and train classifier\n",
        "classifier_trainer.load(15)\n",
        "#classifier_trainer.train(10)\n",
        "# classifier_trainer.update_learning_rate(1e-4)\n",
        "# classifier_trainer.train(5)\n",
        "#classifier_trainer.save()\n",
        "\n",
        "classifier_trainer.test(on='val', max_outputs = 200 )\n",
        "wrong_indices=classifier_trainer.wrong_indices[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MKLCAZBtCCQV"
      },
      "source": [
        "#@title Load and train VAE\n",
        "print(f'model: {model}')\n",
        "print(f'explainability: {explainability}')\n",
        "print(f'vae_version: {vae_version}')\n",
        "\n",
        "vae_trainer.load(50)\n",
        "# for i in range(5):\n",
        "#     lr =initial_learning_rate*.95**vae_trainer.epoch\n",
        "#     vae_trainer.update_learning_rate(lr)\n",
        "#     vae_trainer.train(10)\n",
        "#     vae_trainer.save()\n",
        "vae_trainer.test(on='val', max_outputs = 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "cellView": "form",
        "id": "anfBuebfCCQW"
      },
      "source": [
        "#@title Visualizing Recon and interpolation from Test Dataset\n",
        "if not os.path.exists('fig'):\n",
        "  os.mkdir('fig')\n",
        "ind =  5#@param {type: \"number\"}\n",
        "vae_trainer.visualize_cvae(ind)\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpDyGqxQiH43"
      },
      "source": [
        "#@title VAE loss for samples missclassfied by the classifier\n",
        "vae_trainer.anomaly_detection(wrong_indices)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KdGjKNYDCCQZ"
      },
      "source": [
        "#@title Create a random vector (top latent variable)\n",
        "z_res = (2**(6 - n_resolutions) + 1)**2 #spatial dimention top latent variable\n",
        "channels = settings['z_dims'][-1] #number of channels\n",
        "randsamp = torch.randn(1,z_res*(channels-1)) #condition is deterministic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzV-d8-WCCQZ"
      },
      "source": [
        "# @title Generate sample\n",
        "# Random tensor is fixed\n",
        "condition_on_input =  1 #@param {type: \"number\"}\n",
        "condition_on_z =  5 #@param {type: \"number\"}\n",
        "conditioned_input = torch.FloatTensor([[condition_on_input]])\n",
        "conditioned_z = torch.FloatTensor([z_res*[condition_on_z]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    vae.eval()\n",
        "    if explainability in ['','CPrior','Prior']:\n",
        "      random_output = vae.decoder(dict(hidden=[]), torch.cat([conditioned_z,randsamp],dim=1).to(device))\n",
        "    else:\n",
        "      random_output = vae.decoder(dict(hidden=[]),conditioned_input.to(device),torch.cat([conditioned_z,randsamp],dim=1).to(device))  \n",
        "random_sample = vae_trainer.show(random_output['recon'][0])\n",
        "plt.imshow(random_sample)\n",
        "plt.show\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}